{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PPG'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Baca dataset\n",
    "data = pd.read_csv(\"Dataset/flight_data.csv\")\n",
    "\n",
    "# Pastikan DEP_DELAY adalah tipe data numerik, jika ada nilai yang tidak bisa dikonversi, ganti dengan NaN\n",
    "data[\"DEP_DELAY\"] = pd.to_numeric(data[\"DEP_DELAY\"], errors=\"coerce\")\n",
    "\n",
    "# Kelompokkan data berdasarkan ORIGIN dan hitung rata-rata DEP_DELAY\n",
    "origin_mean_delay = data.groupby(\"ORIGIN\").mean(numeric_only=True)[\"DEP_DELAY\"]\n",
    "\n",
    "# Cari ORIGIN dengan rata-rata keterlambatan keberangkatan terbesar\n",
    "origin_with_max_delay = origin_mean_delay.idxmax()\n",
    "\n",
    "# Output\n",
    "origin_with_max_delay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas digunakan untuk menemukan bandara asal penerbangan yang memiliki keterlambatan keberangkatan rata-rata tertinggi berdasarkan data penerbangan yang disimpan dalam sebuah file CSV. Pertama, dibaca data dari file flight_data.csv dan disimpan ke dalam sebuah DataFrame dengan library pandas. Kemudian, data tersebut dikelompokkan berdasarkan kolom ORIGIN, yang mewakili kode bandara asal. Setelah data dikelompokkan, fungsi mean() digunakan untuk menghitung rata-rata keterlambatan keberangkatan untuk setiap bandara. Kemudian,DEP_DELAY kolom ditarik dari hasil pengelompokan, dan fungsi idxmax() dijalankan untuk mencari bandara asal yang memiliki nilai rata-rata keterlambatan tertinggi. Dengan cara ini, output dari kode ini adalah kode bandara asal dengan nilai rata-rata keterlambatan paling besar pada dataset tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 2]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sentence = [\"How to change payment method and payment frequency\"]\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(sentence).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas menggunakan library scikit-learn untuk memproses teks dan melakukan vektorisasi metode CountVectorizer untuk merubah teks menjadi representasi numerik. Pertama, library scikit-learn diinstal untuk memastikan ketersediaan alat-alat untuk pemrosesan teks. Kemudian, aplikasikan fungsi CountVectorizer guna menghitung kelimpahan kata-kata yang muncul dalam teks. In the sentence \"How to change payment method and payment frequency\" serves as input that will be processed. Parameter stop_words='english' memastikan bahwa kata-kata umum dalam bahasa Inggris (seperti \"to\", \"and\") diabaikan atau dihentikan dalam proses vektorisasi. Fungsi fit_transform() menerapkan vektorisasi pada kalimat tersebut, mengubahnya menjadi matriks fitur berdasarkan frekuensi kata-kata yang tidak termasuk dalam daftar stop words. Terakhir, todense() mengubah representasi matriks menjadi bentuk yang lebih mudah dibaca, yaitu matriks padat (dense matrix), yang akan menampilkan hasil vektorisasi ini sebagai representasi numerik dari kata-kata dalam kalimat tanpa memperhitungkan stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'would', 'have', 'thought', 'that', 'computer', 'programs', 'would', 'be', 'analyzing', 'human', 'sentiments']\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "text = \"Who would have thought that computer programs would be analyzing human sentiments\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas menggunakan library nltk (Natural Language Toolkit) untuk melakukan tokenisasi teks, yaitu proses memecah teks menjadi kata-kata individual. Pertama, kalimat \"Who would have thought that computer programs would be analyzing human sentiments\" disimpan dalam variabel text. Kemudian, modul word_tokenize dari nltk.tokenize digunakan untuk memecah kalimat tersebut menjadi kata-kata yang terpisah, berdasarkan spasi dan tanda baca.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode diatas menggunakan library nltk untuk mengakses daftar stopwords dari bahasa inggris, karena pada dasarnya stopwords sering digunakan untuk  Natural Langugage Processing karena tidak memberikan informasi yang penting dan juga agar dibiarkan ketika melakukan pemrosesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Usain', 'Bolt', 'fastest', 'runner', 'world']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in tokens if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode tersebut adalah sebuah list comprehension di Python yang berfungsi untuk memfilter kata-kata dalam daftar tokens, dengan menghilangkan kata-kata yang terdapat dalam daftar stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'would', 'have', 'thought', 'that', 'computer', 'program', 'would', 'be', 'analyzing', 'human', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "text = \"Who would have thought that computer programs would be analyzing human sentiments\"\n",
    "tokens = word_tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas menggunakan library nltk untuk melakukan lemmatization pada token yang dihasilkan dari sebuah kalimat. Lemmanatization adalah proses membuat suatu kata menjadi bentuk dasarnya atau lemma-nya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'would', 'have', 'thought', 'that', 'comput', 'program', 'would', 'be', 'analyz', 'human', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "text = \"Who would have thought that computer programs would be analyzing human sentiments\"\n",
    "tokens=word_tokenize(text.lower())\n",
    "ps = PorterStemmer()\n",
    "tokens=[ps.stem(word) for word in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas menggunakan library nltk untuk melakukan stemming pada token yang dihasilkan dari sebuah kalimat, yaitu proses mengubah kata menjadi bentuk dasarnya dengan cara menghilangkan akhiran atau sufiks tanpa mempertimbangkan makna kata. Pertama, kalimat \"Who would have thought that computer programs would be analyzing human sentiments\" disimpan dalam variabel text. Kemudian, kalimat tersebut diubah menjadi huruf kecil menggunakan metode .lower() untuk memastikan konsistensi dalam pemrosesan. Setelah itu, fungsi word_tokenize digunakan untuk memecah kalimat menjadi token kata. Selanjutnya, objek PorterStemmer diinisialisasi untuk melakukan stemming. Dengan menggunakan list comprehension, setiap kata dalam daftar tokens diproses dengan memanggil metode stem() dari objek stemmer. Hasilnya adalah daftar kata yang telah di-stem, di mana akhiran yang tidak diperlukan dihapus, dan kata-kata tersebut ditampilkan dengan perintah print(tokens). Misalnya, kata \"analyzing\" diubah menjadi \"analyz\", menunjukkan bagaimana stemming berbeda dari lemmatization dengan tidak selalu menghasilkan kata dasar yang valid dalam bahasa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eat', 'NN')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"your\"])\n",
    "nltk.pos_tag([\"beautiful\"])\n",
    "nltk.pos_tag([\"eat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas menggunakan fungsi pos_tag dari library nltk untuk menandai bagian dari ucapan (part-of-speech tagging) pada kata-kata yang diberikan. Fungsi ini mengidentifikasi jenis kata dalam konteks kalimat, seperti kata benda, kata kerja, kata sifat, dan lainnya. Pertama, fungsi nltk.pos_tag dipanggil dengan argumen berupa daftar yang berisi satu kata: \"your\", menghasilkan tag yang menunjukkan bahwa \"your\" adalah kata ganti kepemilikan (PRP$). Selanjutnya, fungsi yang sama digunakan untuk kata \"beautiful\", yang diidentifikasi sebagai kata sifat (JJ). Terakhir, kata \"eat\" yang diproses akan ditandai sebagai kata kerja (VB). Hasil dari masing-masing pemanggilan fungsi ini adalah daftar pasangan (word, tag) yang menunjukkan kata yang dianalisis dan jenisnya, sehingga memberikan wawasan tentang bagaimana kata tersebut digunakan dalam kalimat. Proses ini penting dalam analisis teks dan pemrosesan bahasa alami karena membantu dalam memahami struktur dan makna kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Usain', 'NN')],\n",
       " [('Bolt', 'NN')],\n",
       " [('is', 'VBZ')],\n",
       " [('the', 'DT')],\n",
       " [('fastest', 'JJS')],\n",
       " [('runner', 'NN')],\n",
       " [('in', 'IN')],\n",
       " [('the', 'DT')],\n",
       " [('world', 'NN')]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Usain Bolt is the fastest runner in the world\"\n",
    "tokens = word_tokenize(text)\n",
    "[nltk.pos_tag([word]) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas melakukan part-of-speech tagging untuk setiap kata dalam kalimat yang diberikan. Pertama, kalimat \"Usain Bolt is the fastest runner in the world\" disimpan dalam variabel text. Kemudian, fungsi word_tokenize digunakan untuk memecah kalimat tersebut menjadi token kata, yang menghasilkan daftar kata-kata individual. Setelah itu, list comprehension digunakan untuk menerapkan fungsi nltk.pos_tag pada setiap kata dalam daftar tokens. Dalam hal ini, setiap kata diubah menjadi daftar yang berisi satu elemen, yaitu kata tersebut, sebelum diproses oleh fungsi pos_tag. Hasilnya adalah daftar dari hasil tag yang menunjukkan bagian dari ucapan untuk setiap kata dalam kalimat, seperti nama diri, kata kerja, kata sifat, dan lainnya. Dengan cara ini, setiap kata dalam kalimat dianalisis untuk memahami perannya dalam konteks kalimat secara keseluruhan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to C:\\Users\\Fa'iq\n",
      "[nltk_data]     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets') # need to download first time\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas digunakan untuk mengunduh dan menampilkan informasi tentang tagset UPenn (Penn Treebank Part-of-Speech Tagset) yang digunakan dalam analisis bagian dari ucapan (part-of-speech tagging) menggunakan library **nltk**. Pertama, perintah `nltk.download('tagsets')` digunakan untuk mengunduh set tag yang diperlukan, yang berisi definisi dan deskripsi dari berbagai tag bagian ucapan yang digunakan dalam corpus **nltk**. Perintah ini biasanya hanya perlu dijalankan sekali untuk memastikan bahwa data yang diperlukan tersedia di sistem. Setelah tagset diunduh, fungsi `nltk.help.upenn_tagset()` dipanggil untuk menampilkan informasi mengenai tagset tersebut. Fungsi ini memberikan daftar tag yang digunakan dalam analisis bagian dari ucapan, beserta penjelasan singkat tentang masing-masing tag, sehingga pengguna dapat memahami makna dan penggunaan setiap tag. Informasi ini sangat berguna bagi para peneliti dan praktisi yang bekerja dengan pemrosesan bahasa alami untuk menganalisis dan memahami struktur kalimat secara lebih mendalam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "TextBlob(\"I love pizza\").sentiment\n",
    "TextBlob(\"The weather is excellent\").sentiment\n",
    "TextBlob(\"What a terrible thing to say\").sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas menggunakan library TextBlob untuk menganalisis sentimen dari beberapa kalimat. TextBlob adalah alat yang kuat untuk pemrosesan bahasa alami yang menyediakan antarmuka sederhana untuk menganalisis teks. Pada setiap baris, objek TextBlob dibuat dengan kalimat yang berbeda, dan properti .sentiment diakses untuk mendapatkan analisis sentimen dari masing-masing kalimat.\n",
    "\n",
    "Analisis sentimen ini menghasilkan dua nilai: polaritas dan subjektivitas. Polaritas berkisar antara -1 (sangat negatif) hingga 1 (sangat positif), yang menunjukkan sikap atau emosi yang terkandung dalam kalimat. Subjektivitas berkisar antara 0 (objektif) hingga 1 (subjektif), yang menunjukkan sejauh mana pernyataan tersebut mencerminkan opini pribadi daripada fakta.\n",
    "\n",
    "Sebagai contoh, kalimat \"I love pizza\" kemungkinan memiliki polaritas tinggi yang positif, sementara kalimat \"What a terrible thing to say\" mungkin memiliki polaritas yang negatif, menunjukkan perasaan ketidakpuasan atau kritik. Dengan demikian, kode ini memberikan wawasan tentang emosi dan opini yang terkandung dalam kalimat-kalimat yang dianalisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation in fr: Qui savait que la traduction pourrait être amusante\n",
      "Translation in zh-CN: 谁知道翻译可能很有趣\n",
      "Translation in hi: कौन जानता था कि अनुवाद मजेदार हो सकता है\n",
      "[('The', 'DT'), ('global', 'JJ'), ('economy', 'NN'), ('is', 'VBZ'), ('expected', 'VBN'), ('to', 'TO'), ('grow', 'VB'), ('this', 'DT'), ('year', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from googletrans import Translator\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Inisialisasi Translator dari googletrans\n",
    "translator = Translator()\n",
    "\n",
    "# List bahasa target\n",
    "languages = ['fr', 'zh-CN', 'hi']\n",
    "\n",
    "# Loop untuk melakukan terjemahan menggunakan googletrans\n",
    "for language in languages:\n",
    "    translated = translator.translate(\"Who knew translation could be fun\", dest=language)\n",
    "    print(f\"Translation in {language}: {translated.text}\")\n",
    "\n",
    "# Melakukan tagging pada teks menggunakan TextBlob\n",
    "blob = TextBlob(\"The global economy is expected to grow this year\")\n",
    "print(blob.tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas menggunakan library **TextBlob** untuk menerjemahkan sebuah kalimat ke dalam beberapa bahasa dan juga untuk melakukan part-of-speech tagging pada kalimat lain. Pertama, daftar `languages` didefinisikan dengan tiga kode bahasa: `'fr'` untuk bahasa Prancis, `'zh-CN'` untuk bahasa Mandarin, dan `'hi'` untuk bahasa Hindi. Dalam loop `for`, kalimat `\"Who knew translation could be fun\"` diterjemahkan ke masing-masing bahasa menggunakan metode `.translate(to=language)` dan hasil terjemahan dicetak ke konsol. Ini menunjukkan kemampuan **TextBlob** untuk melakukan terjemahan otomatis ke dalam berbagai bahasa.\n",
    "\n",
    "Setelah itu, kalimat kedua `\"The global economy is expected to grow this year\"` dianalisis dengan menggunakan metode `.tags`, yang memberikan informasi tentang bagian dari ucapan (part-of-speech) untuk setiap kata dalam kalimat tersebut. Dengan demikian, kode ini tidak hanya mendemonstrasikan kemampuan terjemahan tetapi juga memberikan analisis struktur kalimat melalui tagging, yang berguna dalam pemrosesan bahasa alami untuk memahami makna dan konteks kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.604, 'pos': 0.396, 'compound': 0.5079}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "analyser.polarity_scores(\"This book is very good\")\n",
    "analyser.polarity_scores(\"OMG! The book is so cool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas menggunakan library **VADER** (Valence Aware Dictionary and sEntiment Reasoner) untuk menganalisis sentimen dari kalimat-kalimat tertentu. VADER adalah alat yang dirancang khusus untuk analisis sentimen yang cocok untuk teks yang berasal dari media sosial dan teks informal lainnya. \n",
    "\n",
    "Pertama, objek `SentimentIntensityAnalyzer` diinisialisasi dan disimpan dalam variabel `analyser`. Objek ini digunakan untuk menghitung skor polaritas dari teks yang diberikan. Selanjutnya, metode `polarity_scores()` dipanggil pada objek `analyser` dengan dua kalimat yang berbeda: `\"This book is very good\"` dan `\"OMG! The book is so cool\"`. Metode ini mengembalikan sebuah dictionary yang berisi beberapa skor, termasuk:\n",
    "- **neg**: skor untuk sentimen negatif\n",
    "- **neu**: skor untuk sentimen netral\n",
    "- **pos**: skor untuk sentimen positif\n",
    "- **compound**: skor keseluruhan yang merupakan kombinasi dari ketiga skor di atas, yang menunjukkan tingkat sentimen secara keseluruhan, berkisar antara -1 (sangat negatif) hingga 1 (sangat positif).\n",
    "\n",
    "Dengan demikian, kode ini memungkinkan pengguna untuk memahami seberapa positif atau negatif pernyataan yang diberikan, serta memberikan wawasan tentang emosi yang terkandung dalam kalimat tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\Fa'iq\n",
      "[nltk_data]    |     Brinatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
