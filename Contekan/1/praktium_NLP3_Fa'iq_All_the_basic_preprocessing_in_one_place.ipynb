{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdj1x9R7GeyX"
      },
      "source": [
        "# All the basic preprocessing in one place\n",
        "\n",
        "#### Let's apply all the preprocessing methods we have discussed so far on our Zomato dataset and see how everything works together\n",
        "\n",
        "Fa'iq Zhafran Naufal Brinatta (220535608468 TI/22)\n",
        "@author: Aman Kedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln5KWUcpGeyb",
        "outputId": "269853fe-5c66-47fb-950b-f8535fcec1ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/amankedia/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/amankedia/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZwOXod6Gm_l"
      },
      "source": [
        "Kode tersebut mengimpor pustaka yang diperlukan untuk pemrosesan bahasa alami (NLTK), termasuk stopwords, stemming, dan lemmatization. Ini juga mengimpor pandas untuk manipulasi data dan re untuk ekspresi reguler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo2DV44tGeye",
        "outputId": "ac5b09be-34db-4c7c-a69b-ae67d476b978"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>Virat Kohli did a great thing to open his rest...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>This place have some really heathy options to ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>Aerocity is the most finest place in Delhi for...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review sentiment\n",
              "0  Virat Kohli did a great thing to open his rest...  positive\n",
              "1  This place have some really heathy options to ...  positive\n",
              "2  Aerocity is the most finest place in Delhi for...  positive"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"Dataset/zomato_reviews.csv\")\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwJJ3SgZGeye"
      },
      "outputs": [],
      "source": [
        "corpus = pd.Series(df.Review.tolist()).astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sP4etGhGrPl"
      },
      "source": [
        "Kode ini membaca file CSV bernama \"zomato_reviews.csv\" dari folder \"Dataset\" ke dalam dataframe pandas yang disebut \"df\", dan kemudian menampilkan 3 baris pertama dari dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkCcp92XGeyf",
        "outputId": "3dbd369f-ae3f-41aa-c3cb-9d843a7e8c43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       Virat Kohli did a great thing to open his rest...\n",
              "1       This place have some really heathy options to ...\n",
              "2       Aerocity is the most finest place in Delhi for...\n",
              "3       Yesterday evening there was small team lunch ,...\n",
              "4       I find aerocity to be the best place in delhi ...\n",
              "                              ...                        \n",
              "1591    || DESI LANE || So we were at alipore's most h...\n",
              "1592    \"Desi Lane\" is one of the most trending place ...\n",
              "1593    One of the cool and pocket pinch restaurant at...\n",
              "1594    \"DESI LANE\" one of the best places in town and...\n",
              "1595    Looking for good place for lunch but dont wann...\n",
              "Length: 1596, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbZwEhn4Geyg"
      },
      "source": [
        "### Text Cleaning (Removal of special characters/punctuations & case folding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoN05RMLGeyg"
      },
      "outputs": [],
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhyEyrO0HAbU"
      },
      "source": [
        "Kode di atas membersihkan sebuah kumpulan teks (`corpus`) dengan melakukan beberapa operasi pemrosesan. Untuk setiap baris dalam `corpus`, teks dipisah menjadi kata-kata, lalu jika kata tersebut tidak ada dalam `keep_list`, kode akan menghilangkan karakter non-alfanumerik dan mengubahnya menjadi huruf kecil. Kata yang sudah diolah kemudian disatukan kembali menjadi kalimat. Jika kata ada di dalam `keep_list`, kata tersebut dibiarkan tetap. Semua baris hasil pembersihan digabungkan menjadi `cleaned_corpus` sebagai output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r0mbTIQGeyh"
      },
      "source": [
        "### Stopwords Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaEokeYBGeyh"
      },
      "outputs": [],
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH4CU13aHGu9"
      },
      "source": [
        "Fungsi `stopwords_removal` menghapus kata-kata umum (stopwords) dari sebuah kumpulan teks (`corpus`). Fungsi ini menggunakan daftar stopwords bahasa Inggris dari pustaka NLTK, tetapi tetap mempertahankan kata-kata tanya (WH-words) seperti 'who', 'what', 'when', dll., dengan menghapusnya dari daftar stopwords. Setelah itu, setiap kata dalam kalimat yang ada di `corpus` akan dihapus jika merupakan stopword, dan teks yang telah dibersihkan dikembalikan dalam bentuk daftar kata-kata tanpa stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSVktPEvGeyi"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdwqRYX2Geyi"
      },
      "outputs": [],
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzNTTEC2HM3U"
      },
      "source": [
        "Fungsi `lemmatize` melakukan lemmatisasi pada kumpulan teks (`corpus`). Menggunakan objek `WordNetLemmatizer` dari pustaka NLTK, fungsi ini mengubah setiap kata dalam `corpus` menjadi bentuk dasarnya (lemma), dengan menganggap setiap kata sebagai sebuah kata kerja (pos = 'v'). Hasilnya adalah kumpulan teks yang telah mengalami lemmatisasi, di mana kata-kata dikembalikan ke bentuk dasar atau kata dasarnya, seperti mengubah \"running\" menjadi \"run\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iswo9GsRGeyi"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrPZr3ZXGeyj"
      },
      "outputs": [],
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFut9ruvHWxH"
      },
      "source": [
        "Fungsi `stem` melakukan stemming pada kumpulan teks (`corpus`), yaitu proses mengubah kata menjadi bentuk dasarnya. Fungsi ini menggunakan dua jenis stemmer: jika parameter `stem_type` disetel ke `'snowball'`, maka digunakan `SnowballStemmer` untuk bahasa Inggris. Jika tidak, fungsi menggunakan `PorterStemmer` sebagai default. Kedua stemmer ini memotong akhir kata untuk menghasilkan bentuk dasar (stem). Hasil stemming ini kemudian dikembalikan sebagai corpus yang telah diolah."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32uVxfRGGeyj"
      },
      "outputs": [],
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa63oleIHgCz"
      },
      "source": [
        "Fungsi `preprocess` melakukan serangkaian tugas pemrosesan teks pada kumpulan teks (`corpus`), seperti pembersihan, penghapusan stopwords, stemming, dan lemmatisasi. Parameter `keep_list` menentukan kata-kata yang harus dipertahankan selama pembersihan. Jika `cleaning` diaktifkan, fungsi membersihkan teks menggunakan fungsi `text_clean`. Jika `remove_stopwords` aktif, stopwords dihapus menggunakan `stopwords_removal`. Pengguna dapat memilih antara stemming (dengan opsi Porter atau Snowball stemmer) dan lemmatisasi, tetapi tidak keduanya bersamaan. Hasil akhir dikembalikan sebagai teks yang telah diproses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd8wa2GSGeyj"
      },
      "outputs": [],
      "source": [
        "common_dot_words = ['U.S.A', 'Mr.', 'Mrs.', 'D.C.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkFO82lvGeyj"
      },
      "outputs": [],
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "corpus_with_lemmatization = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None, lemmatization = True, remove_stopwords = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDJ2shn4Geyk"
      },
      "outputs": [],
      "source": [
        "# Preprocessing with Stemming here here\n",
        "corpus_with_stemming = preprocess(corpus, keep_list = common_dot_words, stemming = True, stem_type = \"snowball\", lemmatization = False, remove_stopwords = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn3amac1HiPf"
      },
      "source": [
        "Pada kode di atas, dua preprocessing dilakukan pada kumpulan teks (`corpus`). Yang pertama, `corpus_with_lemmatization`, menggunakan fungsi `preprocess` untuk menerapkan lemmatisasi dengan mempertahankan kata-kata tertentu yang mengandung titik (seperti 'U.S.A', 'Mr.', 'Mrs.', 'D.C.') dan menghapus stopwords, tanpa menggunakan stemming. Yang kedua, `corpus_with_stemming` juga memproses `corpus` tetapi dengan menggunakan stemming (menggunakan Snowball stemmer) dan tetap menghapus stopwords, sambil tidak menerapkan lemmatisasi. Kedua hasil dipisahkan ke dalam variabel yang berbeda untuk analisis lebih lanjut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYdff0SlGeyk"
      },
      "source": [
        "# Let's see the results on applying\n",
        "\n",
        "### 1. Lemmatization\n",
        "### 2. Stemming\n",
        "\n",
        "Note: Stopwords removal and text cleaning have been applied on both the occassions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tBGHKO2Geyk",
        "outputId": "a7564a4b-44c9-470a-bffc-55e5b0bc7fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original string:  Virat Kohli did a great thing to open his restaurant in an exquisite place of Delhi. Wide range of food with lots and lots of options on drinks. Courteous staff with a quick response on anything.\n"
          ]
        }
      ],
      "source": [
        "print(\"Original string: \", corpus[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoGStVfFGeyk",
        "outputId": "24c8f328-356e-48cb-82db-ca416e21b71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "String after lemmatiization:  virat kohli great thing open restaurant exquisite place delhi wide range food lot lot options drink courteous staff quick response anything\n"
          ]
        }
      ],
      "source": [
        "print(\"String after lemmatiization: \", corpus_with_lemmatization[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhvWkW5sGeyk",
        "outputId": "3c8d8b0f-fae2-42e7-c77e-23f8430b0dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "String after stemming:  virat koh great thing open restaur exquisit place delhi wide rang food lot lot option drink courteous staff quick respons anyth\n"
          ]
        }
      ],
      "source": [
        "print(\"String after stemming: \", corpus_with_stemming[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4kPKUDNICFs"
      },
      "source": [
        "Perbandingan antara string asli dan hasil lemmatisasi serta stemming menunjukkan perbedaan signifikan dalam keterbacaan dan makna. Lemmatisasi mempertahankan bentuk dasar kata dengan lebih baik, seperti \"options\" menjadi \"option\", sehingga konteks dan nuansa kalimat tetap jelas. Sebaliknya, stemming mengubah kata-kata menjadi bentuk yang lebih pendek dan sering kali tidak sesuai, seperti \"courteous\" menjadi \"courtes\" dan \"response\" menjadi \"respons\", yang dapat mengaburkan makna. Secara keseluruhan, lemmatisasi lebih efektif dalam menjaga makna dan keterbacaan, sementara stemming lebih fokus pada penyederhanaan tanpa mempertimbangkan konteks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1_QW3CkICYO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
