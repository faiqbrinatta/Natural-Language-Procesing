{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZOaxY7TILiE"
      },
      "source": [
        "# Exploring Tokenization\n",
        "\n",
        "Fa'iq Zhafran Naufal Brinatta (220535608468 TI/22)\n",
        "@author: Aman Kedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d4L4U9JGILiI"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KQx2vZH0ILiJ"
      },
      "outputs": [],
      "source": [
        "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
        "           'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-pe2r5oIhVo"
      },
      "source": [
        "Baris kode tersebut adalah sebuah daftar bernama `plurals`, yang berisi kata-kata dalam bentuk jamak (plural) dalam bahasa Inggris, seperti \"caresses\", \"flies\", dan \"died\". Daftar ini dapat digunakan dalam analisis teks untuk mengidentifikasi dan menangani kata-kata jamak yang mungkin perlu diproses secara khusus, misalnya diubah menjadi bentuk tunggal selama proses stemming atau lemmatization. Selain itu, daftar ini berhubungan dengan teknik lain dalam pengolahan teks, seperti stopword removal, case-folding, n-grams, dan penghapusan HTML tags, yang semuanya bertujuan untuk membersihkan dan memperbaiki kualitas analisis teks. Dengan menandai kata-kata jamak dalam data yang dianalisis, kita dapat memastikan bahwa hasil analisis lebih akurat dan bermakna."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxJYuSe3ILiK"
      },
      "source": [
        "# Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jMwsWSwAILiK",
        "outputId": "63b80435-eeb0-4484-f494-e68dabf866c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "singles = [stemmer.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7d1KY-PIrQN"
      },
      "source": [
        "Kode ini menggunakan pustaka Natural Language Toolkit (NLTK) di Python untuk melakukan stemming pada kata-kata dalam daftar `plurals`. Pertama, `PorterStemmer` diimpor dari modul `nltk.stem.porter`, dan kemudian sebuah objek `stemmer` dibuat dari kelas tersebut. Menggunakan list comprehension, metode `stem()` diterapkan pada setiap kata dalam daftar `plurals`, menghasilkan daftar baru bernama `singles`, di mana setiap kata jamak diubah menjadi bentuk dasarnya. Akhirnya, hasil stemming dicetak dalam format string, dengan setiap kata dipisahkan oleh spasi. Secara keseluruhan, kode ini bertujuan untuk mengubah kata-kata dalam bentuk jamak menjadi bentuk dasar mereka, memudahkan analisis lebih lanjut pada data teks dan memberikan insight yang lebih baik dalam konteks model pembelajaran mesin atau analisis data lainnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT3GO9M_ILiL"
      },
      "source": [
        "# Snowball Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8c7TOp7iILiM",
        "outputId": "6696100c-7c88-4d03-bead-9e4ce340a6ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "print(SnowballStemmer.languages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PTmRLv99ILiM",
        "outputId": "cdcbfd4b-e286-4094-bf2d-10a97ac8f481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have generous\n"
          ]
        }
      ],
      "source": [
        "stemmer2 = SnowballStemmer(language='english')\n",
        "singles = [stemmer2.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bLXtR9_I2N9"
      },
      "source": [
        "Kode ini menggunakan `SnowballStemmer` dari pustaka NLTK untuk melakukan stemming pada kata-kata dalam daftar `plurals` dengan fokus pada bahasa Inggris. Pertama, sebuah objek `stemmer2` dibuat dengan menentukan bahasa sebagai 'english', yang mengindikasikan bahwa algoritma Snowball akan digunakan untuk stemming kata dalam konteks bahasa Inggris. Selanjutnya, list comprehension diterapkan untuk menerapkan metode `stem()` dari objek `stemmer2` pada setiap kata dalam daftar `plurals`, menghasilkan daftar baru bernama `singles`, di mana setiap kata jamak diubah menjadi bentuk dasarnya. Akhirnya, hasil stemming dicetak dalam format string, dengan setiap kata dipisahkan oleh spasi. Dengan demikian, kode ini bertujuan untuk memberikan hasil stemming yang lebih akurat dan relevan dalam konteks analisis teks berbahasa Inggris, memanfaatkan keunggulan algoritma Snowball dibandingkan dengan algoritma lainnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEmTotPRILiN"
      },
      "source": [
        "# Wordnet Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mWo_n4LZILiN",
        "outputId": "afb839d1-e703-4de5-aebe-5f395540bc3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Fa'iq\n",
            "[nltk_data]     Brinatta\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DLpStsPaILiN",
        "outputId": "db3870d0-a4c7-4b4b-fd32-dc493d6c8056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
            "The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "token_list = s.split()\n",
        "print(\"The tokens are: \", token_list)\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token in token_list])\n",
        "print(\"The lemmatized output is: \", lemmatized_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uylooOfI9Bn"
      },
      "source": [
        "Kode ini menggunakan pustaka NLTK untuk melakukan lemmatization pada sebuah kalimat yang disimpan dalam variabel `s`. Pertama, pustaka `wordnet` diunduh untuk mendukung lemmatization, dan objek `lemmatizer` dibuat dari kelas `WordNetLemmatizer`. Kalimat dalam variabel `s` dipecah menjadi token menggunakan metode `split()`, yang menghasilkan daftar kata yang disimpan dalam `token_list`. Kode kemudian mencetak daftar token tersebut. Selanjutnya, list comprehension diterapkan untuk menerapkan metode `lemmatize()` dari objek `lemmatizer` pada setiap token dalam `token_list`, menghasilkan output lemmatized yang kemudian digabungkan kembali menjadi satu string menggunakan `join()`. Hasil akhir mencetak kalimat yang sudah melalui proses lemmatization, di mana kata-kata diubah menjadi bentuk dasarnya, sehingga menghasilkan output yang lebih bersih dan terstandarisasi untuk analisis lebih lanjut dalam pengolahan bahasa alami."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9mDqdQpILiO"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lUOf8i20ILiO",
        "outputId": "ed21ccbf-083a-498c-833e-308d35fa35a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Fa'iq Brinatta\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('We', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('putting', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('efforts', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('enhance', 'VB'),\n",
              " ('our', 'PRP$'),\n",
              " ('understanding', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('Lemmatization', 'NN')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tags = nltk.pos_tag(token_list)\n",
        "pos_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paXGTPUHJCdr"
      },
      "source": [
        "POS tagging (Part-of-Speech tagging) adalah proses mengidentifikasi dan menandai jenis kata (part of speech) dalam sebuah kalimat, seperti kata benda (noun), kata kerja (verb), kata sifat (adjective), dan lain-lain. Pada kode ini, pustaka NLTK diunduh untuk menggunakan model \"averaged_perceptron_tagger\", yang merupakan salah satu algoritma yang digunakan untuk melakukan tagging ini. Setelah itu, fungsi `nltk.pos_tag()` diterapkan pada daftar token `token_list`, yang sebelumnya dihasilkan dari kalimat. Fungsi ini mengembalikan daftar pasangan, di mana setiap pasangan terdiri dari sebuah token dan label POS-nya. Misalnya, untuk token \"Lemmatization\", hasil tagging mungkin menunjukkan bahwa itu adalah kata benda. Hasil dari POS tagging ini sangat berguna dalam analisis teks dan pengolahan bahasa alami, karena membantu dalam memahami fungsi kata dalam kalimat, yang selanjutnya dapat digunakan untuk tugas-tugas lebih lanjut seperti sintaksis, analisis semantik, atau pemrosesan teks berbasis konteks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3V09KbsILiO"
      },
      "source": [
        "## POS tag Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w2bcvgsRILiO"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
        "\n",
        "def get_part_of_speech_tags(token):\n",
        "\n",
        "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
        "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
        "\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyqkT0NEJH3P"
      },
      "source": [
        "Fungsi `get_part_of_speech_tags` dalam kode ini dirancang untuk memetakan label POS (Part-of-Speech) dari sebuah token ke dalam format yang dapat diterima oleh fungsi `lemmatize()` dari pustaka WordNet. Fungsi ini fokus pada empat jenis kata utama: kata sifat (adjective), kata benda (noun), kata kerja (verb), dan kata keterangan (adverb). Di dalam fungsi, `tag_dict` adalah kamus yang mengaitkan karakter pertama dari label POS dengan konstanta yang sesuai dari WordNet. Fungsi ini menggunakan `nltk.pos_tag()` untuk mendapatkan label POS dari token yang diberikan, lalu mengambil karakter pertama dari tag tersebut dan mengubahnya menjadi huruf besar. Kemudian, fungsi mengembalikan nilai dari `tag_dict` berdasarkan tag yang diidentifikasi; jika tag tidak ditemukan, nilai default yang dikembalikan adalah `wordnet.NOUN`. Dengan cara ini, fungsi ini memfasilitasi penggunaan lemmatization yang lebih akurat berdasarkan konteks kata dalam kalimat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6viYXlXhILiO"
      },
      "source": [
        "## Wordnet Lemmatizer with POS Tag Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GwNAGdKTILiO",
        "outputId": "ef7e9fff-2746-4175-9802-1742139f9677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We be put in effort to enhance our understand of Lemmatization\n"
          ]
        }
      ],
      "source": [
        "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
        "print(' '.join(lemmatized_output_with_POS_information))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmGs94UiJNl8"
      },
      "source": [
        "Kode ini menggunakan fungsi `lemmatizer.lemmatize()` bersama dengan informasi part-of-speech (POS) untuk melakukan lemmatization yang lebih kontekstual pada daftar token yang dihasilkan sebelumnya dari kalimat. Dalam list comprehension, setiap token dalam `token_list` diproses dengan memanggil `get_part_of_speech_tags(token)` untuk menentukan jenis kata yang sesuai, seperti kata benda, kata kerja, kata sifat, atau kata keterangan. Dengan menggunakan informasi POS yang tepat, `lemmatizer` dapat menghasilkan bentuk dasar (lemma) dari setiap token dengan lebih akurat, karena lemmatization mempertimbangkan konteks kata dalam kalimat. Hasil akhir, yaitu daftar kata yang sudah di-lemmatize, digabungkan kembali menjadi satu string dengan menggunakan `join()`, dan kemudian dicetak. Proses ini membantu memastikan bahwa kata-kata dalam output lemmatized lebih relevan dan sesuai dengan makna yang dimaksud dalam kalimat aslinya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAZTZHonILiP"
      },
      "source": [
        "## Lemmatization vs Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NlslGlNAILiP",
        "outputId": "ea9b6871-6933-4b59-fc45-6283d54e57ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we are put in effort to enhanc our understand of lemmat\n"
          ]
        }
      ],
      "source": [
        "stemmer2 = SnowballStemmer(language='english')\n",
        "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
        "print(' '.join(stemmed_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SloT-onZqtzj"
      },
      "source": [
        "Kode tersebut melakukan stemming pada setiap kata dalam token_list menggunakan algoritma SnowballStemmer yang dikhususkan untuk bahasa Inggris. Objek stemmer2 dibuat untuk keperluan ini. Kemudian, setiap kata dalam token_list diproses menggunakan metode stem() dari objek stemmer2, dan hasilnya disimpan dalam list stemmed_sentence. Akhirnya, semua kata dalam stemmed_sentence digabungkan dengan spasi dan dicetak, menunjukkan hasil stemming dari kalimat awal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-HWANUJILiP"
      },
      "source": [
        "# spaCy Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VJSczLKhILiP",
        "outputId": "dc8b8e96-790d-4188-9b36-4d56a3a92195"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\Fa'iq Brinatta\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the downloaded model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\compat.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\config.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\types.py:25\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     Any,\n\u001b[0;32m      6\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     overload,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy, has_cupy\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[0;32m     28\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\compat.py:35\u001b[0m\n\u001b[0;32m     31\u001b[0m     has_cupy_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlpack\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     has_torch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
            "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\Fa'iq Brinatta\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the downloaded model\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# Example usage\n",
        "doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n",
        "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "print(lemmatized_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSL5vnWWrABC"
      },
      "source": [
        "Kode tersebut melakukan lemmatisasi pada kalimat \"We are putting in efforts to enhance our understanding of Lemmatization\" menggunakan library spaCy. Pertama, model bahasa Inggris 'en' dimuat ke dalam objek nlp. Kemudian, kalimat diproses menggunakan nlp untuk membuat objek doc. Objek doc ini berisi informasi linguistik tentang kalimat, termasuk lemma dari setiap token. Terakhir, kode tersebut mengekstrak lemma dari setiap token dalam doc menggunakan atribut token.lemma_ dan menggabungkannya menjadi satu string, yang kemudian dicetak, menunjukkan hasil lemmatisasi dari kalimat tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69ULD9b0ILiQ"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LC5qpxlhILiQ",
        "outputId": "6a953941-d569-43dd-cb39-682f21d6df32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Fa'iq\n",
            "[nltk_data]     Brinatta\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"hasn, re, wasn't, ourselves, from, at, she's, couldn't, who, your, himself, hers, were, about, aren't, until, just, haven, before, did, after, mustn, through, had, over, aren, theirs, such, being, won't, mightn't, to, where, too, hadn't, when, all, any, both, ain, didn't, further, above, m, ours, so, they, now, her, each, t, yourselves, been, having, those, my, shan't, don, didn, how, shan, nor, doesn't, which, hasn't, more, be, some, i, on, itself, why, off, same, isn't, their, yourself, under, does, own, you're, isn, it, out, than, other, only, should've, have, herself, she, in, can, couldn, his, are, not, haven't, weren, once, doesn, d, for, you've, very, mustn't, with, down, you, that'll, them, by, shouldn't, that, won, was, is, needn, ma, has, between, doing, most, will, below, no, wouldn't, shouldn, am, the, a, few, don't, he, y, it's, of, during, should, do, mightn, themselves, its, if, or, against, but, as, s, yours, whom, we, wasn, ll, into, weren't, our, then, o, while, you'd, these, me, what, up, needn't, and, this, here, there, myself, an, wouldn, him, you'll, ve, hadn, again, because\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "\", \".join(stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuyvE-9wrG_q"
      },
      "source": [
        "Kode tersebut mengunduh daftar stop words bahasa Inggris dari library NLTK dan menampilkannya. Pertama, nltk.download('stopwords') mengunduh data stop words jika belum ada. Kemudian, from nltk.corpus import stopwords mengimpor modul stopwords. Selanjutnya, stop = set(stopwords.words('english')) membuat sebuah set bernama stop yang berisi stop words bahasa Inggris. Terakhir, \", \".join(stop) menggabungkan semua stop words dalam stop menjadi satu string dengan setiap kata dipisahkan oleh koma dan spasi, lalu menampilkan string tersebut."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-3uVX35mILiQ",
        "outputId": "d29ab541-d420-4569-d7c6-fe0ad8270e0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'how putting efforts enhance understanding Lemmatization'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
        "\n",
        "for word in wh_words:\n",
        "    stop.remove(word)\n",
        "\n",
        "sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
        "\" \".join(sentence_after_stopword_removal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NyU5JofrO0_"
      },
      "source": [
        "Kode di atas melakukan penghapusan kata-kata tanya (wh-words) dan kata-kata umum (stopwords) dari sebuah kalimat berbahasa Inggris. Pertama, daftar kata tanya seperti 'who', 'what', 'when', 'why', 'how', 'which', 'where', dan 'whom' disimpan dalam variabel `wh_words`. Kemudian, menggunakan pustaka `stopwords` dari NLTK, kata-kata umum dalam bahasa Inggris dimasukkan ke dalam sebuah set bernama `stop`. Dalam kalimat \"how are we putting in efforts to enhance our understanding of Lemmatization\", setiap kata tanya dihapus dari set `stop`. Akhirnya, kalimat tersebut dipecah menjadi token, dan hanya token yang tidak termasuk dalam set `stop` yang disimpan dalam daftar baru, `sentence_after_stopword_removal`. Hasilnya adalah kalimat yang telah dibersihkan dari kata-kata umum, yang dapat dilihat melalui penggabungan kembali token tersebut menjadi string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ffp2acOILiQ"
      },
      "source": [
        "# Case Folding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "udj3oJDxILiQ",
        "outputId": "2c7151ad-187e-457d-e193-bfa9681369e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we are putting in efforts to enhance our understanding of lemmatization'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "s = s.lower()\n",
        "s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTK5SVw2rTxP"
      },
      "source": [
        "Kode di atas mengambil string `s`, yang berisi kalimat \"We are putting in efforts to enhance our understanding of Lemmatization\", dan mengubah semua karakter dalam string tersebut menjadi huruf kecil menggunakan metode `lower()`. Dengan melakukan ini, hasilnya adalah string yang lebih konsisten dan mudah diproses, terutama dalam konteks pemrosesan teks dan analisis, di mana perbedaan antara huruf besar dan kecil sering kali diabaikan. Setelah pengubahan, string yang dihasilkan adalah \"we are putting in efforts to enhance our understanding of lemmatization\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpmOyLIFILiR"
      },
      "source": [
        "# N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IcG3GE1EILiR",
        "outputId": "15f0231d-8355-4f3b-d023-e7e1aeef0c8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Natural Language',\n",
              " 'Language Processing',\n",
              " 'Processing is',\n",
              " 'is the',\n",
              " 'the way',\n",
              " 'way to',\n",
              " 'to go']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = s.split()\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "[\" \".join(token) for token in bigrams]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gteYIny1rawC"
      },
      "source": [
        "Kode di atas menghasilkan bigram dari kalimat \"Natural Language Processing is the way to go\". String tersebut dipecah menjadi token menggunakan `split()`, lalu fungsi `ngrams` dari NLTK membuat pasangan dua kata. Akhirnya, setiap bigram digabungkan kembali menjadi string, menghasilkan daftar bigram: [\"Natural Language\", \"Language Processing\", \"Processing is\", \"is the\", \"the way\", \"way to\", \"to go\"]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DEX7EQHNILiR",
        "outputId": "6705f11a-51d5-4fa1-fc40-a7b8b09b888f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Natural Language Processing',\n",
              " 'Language Processing is',\n",
              " 'Processing is the',\n",
              " 'is the way',\n",
              " 'the way to',\n",
              " 'way to go']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = s.split()\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "[\" \".join(token) for token in trigrams]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYeRBZyEre8V"
      },
      "source": [
        "Kode di atas menghasilkan bigram dari kalimat \"Natural Language Processing is the way to go\" dengan memecah kalimat menjadi token menggunakan metode `split()`. Kemudian, fungsi `ngrams` dari pustaka NLTK digunakan untuk membuat pasangan dua kata (bigrams) dari token yang dihasilkan. Setiap bigram, yang merupakan dua kata berurutan, digabungkan kembali menjadi string dengan menggunakan metode `join`. Hasil akhirnya adalah daftar bigram yang terdiri dari: [\"Natural Language\", \"Language Processing\", \"Processing is\", \"is the\", \"the way\", \"way to\", \"to go\"]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcrVztFEILiR"
      },
      "source": [
        "# Building a basic vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OW0kYQBgILiR",
        "outputId": "6d761b38-e96c-4288-f2ba-51ddf70bf881"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Language', 'Natural', 'Processing', 'go', 'is', 'the', 'to', 'way']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = set(s.split())\n",
        "vocabulary = sorted(tokens)\n",
        "vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kcwdLHRrjBN"
      },
      "source": [
        "Kode di atas menghasilkan kosakata unik dari kalimat \"Natural Language Processing is the way to go\". Pertama, kalimat tersebut dipecah menjadi token menggunakan metode `split()`, yang memisahkan kata berdasarkan spasi. Kemudian, `set` digunakan untuk menghapus duplikasi dan hanya menyimpan kata-kata unik. Setelah itu, fungsi `sorted` mengurutkan kosakata yang telah dikumpulkan secara alfabetis. Hasil akhirnya adalah daftar terurut dari kata-kata unik dalam kalimat, yang disimpan dalam variabel `vocabulary`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVF3r9-iILiS"
      },
      "source": [
        "# Removing HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qDLHDNiLILiS",
        "outputId": "d8243d28-7240-4d1a-affe-bc9847c6fda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My First HeadingMy first paragraph.\n"
          ]
        }
      ],
      "source": [
        "html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(html)\n",
        "text = soup.get_text()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhLQ1ei9rm5l"
      },
      "source": [
        "Kode di atas menggunakan pustaka BeautifulSoup untuk mengekstrak teks dari string HTML yang diberikan. Pertama, string HTML yang berisi struktur dasar halaman web diinisialisasi dalam variabel `html`. Kemudian, objek BeautifulSoup dibuat dengan mengolah string HTML tersebut. Dengan menggunakan metode `get_text()`, semua teks dalam elemen HTML diekstraksi, sehingga hanya teks yang terlihat, seperti \"My First Heading\" dan \"My first paragraph.\", yang diambil tanpa markup HTML. Hasilnya adalah string yang berisi teks bersih dari konten HTML, yang kemudian dicetak ke layar."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
