{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HquNDsebr9T-"
      },
      "source": [
        "# Exploring Tokenization\n",
        "\n",
	"Mahesa Yuztar (220535601516/TI-B 2022)",
        "@author: Aman Kedia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACcY6p0Tr9UA",
        "outputId": "3f7d65d4-1fb8-4bcd-fe03-1e633597cfa7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The', 'capital', 'of', 'China', 'is', 'Beijing']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"The capital of China is Beijing\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hujSkzAHr9UB",
        "outputId": "c3f6b988-92c9-4476-e5b8-f4117c1d24ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"China's\", 'capital', 'is', 'Beijing']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"China's capital is Beijing\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcn9y8jfr9UB",
        "outputId": "e2d9a077-6808-4643-aa73-8b764ceb59a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Beijing', 'is', 'where', \"we'll\", 'go']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"Beijing is where we'll go\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spVcv-Yyr9UC",
        "outputId": "1b427fd8-2ba6-49dc-b3f1-0d2076148fa7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"I'm\", 'going', 'to', 'travel', 'to', 'Beijing']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"I'm going to travel to Beijing\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ieyhe5tar9UC",
        "outputId": "841e46ac-49d3-4bbe-c9a2-0aa65cdf0df2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Most', 'of', 'the', 'times', 'umm', 'I', 'travel']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"Most of the times umm I travel\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo6Bmxo8r9UD",
        "outputId": "560cb4ee-52ba-486f-813e-c13f326dd2e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Let's\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"Let's travel to Hong Kong from Beijing\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb4rAPJpr9UD",
        "outputId": "7a837037-4128-4144-9dbe-d10958a47d11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"A friend is pursuing his M.S from Beijing\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAYXSy76r9UE",
        "outputId": "bbf16242-f38f-4cf2-97f5-9e9922c1b6ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Beijing', 'is', 'a', 'cool', 'place!!!', ':-P', '<3', '#Awesome']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"Beijing is a cool place!!! :-P <3 #Awesome\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9KJhecSr9UE"
      },
      "source": [
        "# Regexp Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH5dD0W6r9UF",
        "outputId": "01322ec6-bb66-4b17-ec59-9f7a941fedbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$3000.0',\n",
              " '-',\n",
              " '$8000.0',\n",
              " 'in',\n",
              " 'USA',\n",
              " '.']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
        "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFDPzQJr9UH"
      },
      "source": [
        "# Blankline Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDB9xyAor9UI",
        "outputId": "9ae01524-f7d5-489e-9ff1-a30b79e4299f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.',\n",
              " 'I want a book as well']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import BlanklineTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n\\n I want a book as well\"\n",
        "tokenizer = BlanklineTokenizer()\n",
        "tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igiuBnhfr9UI"
      },
      "source": [
        "# WordPunct Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFQCLtVCr9UI",
        "outputId": "5c8f859d-c408-4937-90d6-685dd1fd005c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$',\n",
              " '3000',\n",
              " '.',\n",
              " '0',\n",
              " '-',\n",
              " '$',\n",
              " '8000',\n",
              " '.',\n",
              " '0',\n",
              " 'in',\n",
              " 'USA',\n",
              " '.',\n",
              " 'I',\n",
              " 'want',\n",
              " 'a',\n",
              " 'book',\n",
              " 'as',\n",
              " 'well']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n I want a book as well\"\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRGmfzNZr9UJ"
      },
      "source": [
        "# TreebankWord Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mvIceykr9UK",
        "outputId": "63f9a71f-4399-4fc6-c363-5e7a546aac6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'m\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'which',\n",
              " 'does',\n",
              " \"n't\",\n",
              " 'cost',\n",
              " 'more',\n",
              " 'than',\n",
              " '$',\n",
              " '3000.0']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"I'm going to buy a Rolex watch which doesn't cost more than $3000.0\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk2Irp-Pr9UK"
      },
      "source": [
        "# Tweet Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQKXy6Pvr9UK",
        "outputId": "c4af6121-1a3f-44c6-a4ec-355f7fea7b23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['@amankedia',\n",
              " \"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxxxxxxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer()\n",
        "tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftXnOD2-r9UL",
        "outputId": "66bdc346-598c-4cbd-870a-914b7d7b5d67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Penjelasan"
      ],
      "metadata": {
        "id": "H6qxGeG-soAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berikut penjelasan mengenai pembelajaran Tokenisasi (Tokenization) ini:\n",
        "\n",
        "1. **Pengenalan Tokenisasi**:\n",
        "   Notebook dimulai dengan sel markdown sederhana yang memperkenalkan konsep tokenisasi. Diberikan gambaran tentang tokenisasi, yaitu proses memecah teks (kalimat atau paragraf) menjadi unit-unit yang lebih kecil seperti kata, frasa, atau karakter. Dalam konteks notebook ini, fokusnya adalah pada tokenisasi di tingkat kata.\n",
        "\n",
        "2. **Tokenisasi Dasar Menggunakan `split()`**:\n",
        "   Inti dari notebook ini berpusat pada penggunaan fungsi bawaan Python, yaitu `split()`, yang memecah kalimat menjadi kata-kata berdasarkan spasi. Misalnya, kalimat \"The capital of China is Beijing\" dipecah menjadi `['The', 'capital', 'of', 'China', 'is', 'Beijing']`. Beberapa sel kode menunjukkan hal ini untuk berbagai kalimat, yang menyoroti bagaimana tokenisasi sederhana dapat dilakukan. Setiap kalimat diproses dengan langsung memanggil `split()` pada string kalimat, dan hasil tokennya dicetak.\n",
        "\n",
        "3. **Menangani Kasus Khusus dalam Tokenisasi**:\n",
        "   Contoh-contoh berikutnya mengeksplorasi kalimat yang lebih kompleks yang mengandung kontraksi atau karakter khusus seperti apostrof. Misalnya, kalimat \"Beijing is where we'll go\" di-tokenisasi menjadi `['Beijing', 'is', 'where', 'we\\'ll', 'go']`. Ini menunjukkan bahwa pemisahan dasar bekerja dengan baik untuk kalimat sederhana tetapi mungkin memerlukan teknik yang lebih canggih untuk menangani tanda baca, kontraksi, dan nuansa lain dalam pemrosesan bahasa alami. Namun, notebook ini tampaknya tidak membahas teknik tokenisasi yang lebih canggih selain menggunakan `split()` pada tahap ini."
      ],
      "metadata": {
        "id": "0366BGCWsrAg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}